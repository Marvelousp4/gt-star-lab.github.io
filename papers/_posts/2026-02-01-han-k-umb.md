---

layout: paper

title: Going with the Flow&#58; Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity

image: /papers/images/2026-han-k-ubm.png

authors: Yunhai Han, Linhao Bai, Ziyu Xiao, Zhaodong Yang, Yogita Choudhary, Krishna Jha, Chuizheng Kong, Shreyas Kousik, Harish Ravichandar

year: 2026

journal: Preprint

ref: Han et al., Preprint, 2026.

arxiv: "2602.07413"

github: GT-STAR-Lab/K-UBM

---

## TL;DR
We introduce i) Unified Behavioral Models (UBMs): a framework that, unlike typical reactive mappings, learns dexterous manipulation skills by modeling how visual observations and robot actions co-evolve as coupled dynamical systems, and ii) Koopman-UBM: An effective instantiation of UBMs that acts as an implicit planner that enables faster inference, smoother execution (temporal coherence), and flexible online replanning.

## Abstract

There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.